name: Automated Cascade of Benchmarks
description: |
  This workflow runs a series of benchmarks in a cascade fashion.
  It is triggered manually. The benchmarks are defined in `benchmark_cascade/benchmark-configs.json`.
  Benchmarks are run in parallel. Instructions can be found in `benchmark_cascade/README.md`.

on:
  workflow_dispatch:
    inputs:
        process_results:
          type: boolean
          description: "Skip all the jobs except for processBenchmarks (true) or run a normal cascade (false)"
          default: false
        workflow_run_ids:
            description: "Comma-separated list of workflow run IDs to process (used when process_results=true or for internal cascade tracking)"
            default: ""
        benchmark_set:
            description: "Benchmark set to run (from benchmark_cascade/benchmark-configs.json). Default is 'single': ghcr/dev, dbpedia-openai-1M-1536-angular, qdrant-rps-m-16-ef-128, feature_flags=true."
            default: "single"
        params_override:
            description: "Override parameters for the benchmark set (JSON format, e.g. '{\"params\": {\"qdrant_version\": [\"ghcr/dev\", \"docker/master\"], \"dataset\": [\"glove-100-angular\"]}}')"
            default: "{}"
        current_batch:
            description: "Current batch index (for cascading, internal use)"
            default: "0"
        machines_per_bench:
            type: boolean
            description: "Create/destroy machines for each benchmark run (true) or reuse machines across batches (false)"
            default: false
        machines_info:
            description: "JSON array of machine pairs to use when machines_per_bench is false, e.g. '[{\"server_name\":\"server-0\",\"client_name\":\"client-0\"},{\"server_name\":\"server-1\",\"client_name\":\"client-1\"}]'"
            default: "[]"
        max_parallel:
            type: number
            description: "Maximum number of parallel jobs to run"
            default: 40

jobs:
  prepareBenchmarks:
    name: Prepare Benchmarks
    runs-on: ubuntu-latest
    if: inputs.process_results == false
    outputs:
      matrix: ${{ steps.prepare.outputs.matrix }}
      has_next_batch: ${{ steps.prepare.outputs.has_next_batch }}
      next_batch: ${{ steps.prepare.outputs.next_batch }}
      benchmark_set: ${{ steps.prepare.outputs.benchmark_set }}
      workflow_run_ids: ${{ steps.prepare.outputs.workflow_run_ids }}
      machines_info: ${{ steps.prepare.outputs.machines_info }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10.12'
      - name: Prepare benchmark matrix
        id: prepare
        run: |
          CONFIG_FILE="benchmark_cascade/benchmark-configs.json"
          BENCHMARK_SET="${{ inputs.benchmark_set }}"
          CURRENT_BATCH=${{ inputs.current_batch || 0 }}
          BATCH_SIZE=${{ inputs.max_parallel || 40 }}

          echo "benchmark_set=$BENCHMARK_SET" >> $GITHUB_OUTPUT

          # Accumulate workflow run IDs
          WORKFLOW_RUN_IDS="${{ inputs.workflow_run_ids }}"
          if [ -n "$WORKFLOW_RUN_IDS" ]; then
            WORKFLOW_RUN_IDS="${WORKFLOW_RUN_IDS},${{ github.run_id }}"
          else
            WORKFLOW_RUN_IDS="${{ github.run_id }}"
          fi
          echo "workflow_run_ids=$WORKFLOW_RUN_IDS" >> "$GITHUB_OUTPUT"

          # Benchmark set - read from config file
          if [ ! -f "$CONFIG_FILE" ]; then
            echo "Configuration file $CONFIG_FILE not found!"
            exit 1
          fi

          BENCHMARK_CONFIGS=$(jq -r ".benchmark_sets.\"$BENCHMARK_SET\"" "$CONFIG_FILE")

          if [ "$BENCHMARK_CONFIGS" = "null" ]; then
            echo "Benchmark set '$BENCHMARK_SET' not found in configuration file"
            exit 1
          fi

          # Apply params_override if provided
          PARAMS_OVERRIDE='${{ inputs.params_override }}'
          if [ "$PARAMS_OVERRIDE" != "{}" ]; then
            echo "Applying params_override: $PARAMS_OVERRIDE"

            # Parse the override params
            OVERRIDE_PARAMS=$(echo "$PARAMS_OVERRIDE" | jq -r '.params // {}')

            if [ "$OVERRIDE_PARAMS" != "{}" ]; then
              # Generate all parameter combinations using separate Python script
              if ! BENCHMARK_CONFIGS=$(python3 benchmark_cascade/generate_configs.py "$OVERRIDE_PARAMS" "$BENCHMARK_CONFIGS"); then
                echo "Error: Failed to generate benchmark configurations"
                exit 1
              fi

              echo "Generated $(echo "$BENCHMARK_CONFIGS" | jq length) configurations after applying params_override"
            fi
          fi

          TOTAL_CONFIGS=$(echo "$BENCHMARK_CONFIGS" | jq length)
          START_INDEX=$((CURRENT_BATCH * BATCH_SIZE))
          END_INDEX=$((START_INDEX + BATCH_SIZE - 1))

          echo "Batch info: Current batch=$CURRENT_BATCH, Start=$START_INDEX, End=$END_INDEX, Total=$TOTAL_CONFIGS"

          # Check if we need more batches
          NEXT_START=$((END_INDEX + 1))
          if [ $NEXT_START -lt $TOTAL_CONFIGS ]; then
            echo "has_next_batch=true" >> $GITHUB_OUTPUT
            echo "next_batch=$((CURRENT_BATCH + 1))" >> $GITHUB_OUTPUT
          else
            echo "has_next_batch=false" >> $GITHUB_OUTPUT
            echo "next_batch=0" >> $GITHUB_OUTPUT
          fi

          # Create matrix for current batch
          MATRIX_JSON="["
          FIRST=true

          for i in $(seq $START_INDEX $END_INDEX); do
            if [ $i -ge $TOTAL_CONFIGS ]; then
              break
            fi

            if [ "$FIRST" = false ]; then
              MATRIX_JSON="${MATRIX_JSON},"
            fi
            FIRST=false

            CONFIG=$(echo "$BENCHMARK_CONFIGS" | jq -r ".[$i]")
            QDRANT_VERSION=$(echo "$CONFIG" | jq -r '.qdrant_version')
            DATASET=$(echo "$CONFIG" | jq -r '.dataset')
            ENGINE_CONFIG=$(echo "$CONFIG" | jq -r '.engine_config')
            FEATURE_FLAGS=$(echo "$CONFIG" | jq -r '.feature_flags_all')

            QDRANT_VERSION_SANITIZED=$(echo "$QDRANT_VERSION" | sed 's|/|-|g')

            MATRIX_JSON="${MATRIX_JSON}{\"index\": $i, \"qdrant_version\": \"$QDRANT_VERSION\", \"qdrant_version_sanitized\": \"$QDRANT_VERSION_SANITIZED\", \"dataset\": \"$DATASET\", \"engine_config\": \"$ENGINE_CONFIG\", \"feature_flags_all\": $FEATURE_FLAGS}"
          done

          MATRIX_JSON="${MATRIX_JSON}]"

          echo "Current batch: $CURRENT_BATCH (indices $START_INDEX-$END_INDEX of $TOTAL_CONFIGS total)"
          echo "Matrix JSON: $MATRIX_JSON"

          echo "matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT

          # Generate machines_info array for the current batch
          MACHINES_INFO="["
          MACHINES_FIRST=true

          for i in $(seq $START_INDEX $END_INDEX); do
            if [ $i -ge $TOTAL_CONFIGS ]; then
              break
            fi

            if [ "$MACHINES_FIRST" = false ]; then
              MACHINES_INFO="${MACHINES_INFO},"
            fi
            MACHINES_FIRST=false

            SERVER_NAME="benchmark-cascade-server-${i}"
            CLIENT_NAME="benchmark-cascade-client-${i}"
            MACHINES_INFO="${MACHINES_INFO}{\"server_name\":\"$SERVER_NAME\",\"client_name\":\"$CLIENT_NAME\"}"
          done

          MACHINES_INFO="${MACHINES_INFO}]"
          echo "Generated machines_info: $MACHINES_INFO"
          echo "machines_info=$MACHINES_INFO" >> $GITHUB_OUTPUT

  setupMachines:
    name: Setup Machines ${{ matrix.config.index }}
    needs: prepareBenchmarks
    if: inputs.process_results == false
    runs-on: ubuntu-latest
    strategy:
      max-parallel: ${{ fromJSON(inputs.max_parallel) }}
      fail-fast: false
      matrix:
        config: ${{ fromJSON(needs.prepareBenchmarks.outputs.matrix) }}
    outputs:
      server_name: ${{ steps.setup.outputs.server_name }}
      client_name: ${{ steps.setup.outputs.client_name }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      - uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
      - name: Set up server and client machines
        id: setup
        run: |
          export HCLOUD_TOKEN=${{ secrets.HCLOUD_TOKEN }}
          export CURRENT_INDEX=${{ matrix.config.index }}
          export CURRENT_BATCH=${{ inputs.current_batch || 0 }}
          export MACHINES_PER_BENCH="${{ inputs.machines_per_bench }}"

          # Determine machine names based on machines_per_bench and batch
          if [ "$MACHINES_PER_BENCH" = "false" ]; then
            # Use machines_info array - from inputs for batch 1+, from prepareBenchmarks for batch 0
            if [ "$CURRENT_BATCH" != "0" ]; then
              echo "machines_per_bench is false and not initial batch (batch=$CURRENT_BATCH), using machines_info from inputs"
              MACHINES_INFO='${{ inputs.machines_info }}'
            else
              echo "machines_per_bench is false and initial batch (batch=$CURRENT_BATCH), using machines_info from prepareBenchmarks"
              MACHINES_INFO='${{ needs.prepareBenchmarks.outputs.machines_info }}'
            fi

            JOB_INDEX=${{ strategy.job-index }}

            # Extract server and client names from machines_info array using jq
            export SERVER_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].server_name")
            export CLIENT_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].client_name")

            if [ "$SERVER_NAME" = "null" ] || [ "$CLIENT_NAME" = "null" ]; then
              echo "Error: Could not find machine info for job index $JOB_INDEX in machines_info"
              echo "Available machines_info: $MACHINES_INFO"
              exit 1
            fi

            # Only create machines on first batch, reuse on subsequent batches
            if [ "$CURRENT_BATCH" = "0" ]; then
              echo "Creating new machines: SERVER_NAME=$SERVER_NAME, CLIENT_NAME=$CLIENT_NAME"
              export BENCH_SERVER_NAME=${SERVER_NAME}
              export BENCH_CLIENT_NAME=${CLIENT_NAME}
            else
              echo "Using existing machines: SERVER_NAME=$SERVER_NAME, CLIENT_NAME=$CLIENT_NAME"
            fi
          else
            echo "Creating new machines per benchmark (machines_per_bench=$MACHINES_PER_BENCH)"
            export SERVER_NAME="benchmark-cascade-server-${CURRENT_INDEX}"
            export CLIENT_NAME="benchmark-cascade-client-${CURRENT_INDEX}"
            export BENCH_SERVER_NAME=${SERVER_NAME}
            export BENCH_CLIENT_NAME=${CLIENT_NAME}
          fi

          # Create machines if needed
          if [ "$MACHINES_PER_BENCH" = "true" ] || [ "$CURRENT_BATCH" = "0" ]; then

            bash -x tools/setup_ci.sh

            # Function to create server with retries
            create_server_with_retries() {
              local server_name=$1
              local server_type=$2
              local max_retries=5
              local retry_count=0

              while [ $retry_count -lt $max_retries ]; do
                echo "Creating server $server_name (attempt $((retry_count + 1))/$max_retries)..."

                if SERVER_NAME=$server_name SERVER_TYPE=$server_type bash -x "tools/hetzner/create_and_install.sh"; then
                  echo "Successfully created server $server_name"
                  return 0
                else
                  retry_count=$((retry_count + 1))
                  if [ $retry_count -lt $max_retries ]; then
                    echo "Failed to create server $server_name, waiting 60 seconds before retry..."
                    sleep 60
                  fi
                fi
              done

              echo "Failed to create server $server_name after $max_retries attempts"
              return 1
            }

            # Create server and client machines with retries in parallel
            create_server_with_retries "$BENCH_SERVER_NAME" "cpx51" &
            SERVER_PID=$!

            create_server_with_retries "$BENCH_CLIENT_NAME" "cpx41" &
            CLIENT_PID=$!

            # Wait for both to complete and check their exit statuses
            wait $SERVER_PID
            SERVER_EXIT_CODE=$?

            wait $CLIENT_PID
            CLIENT_EXIT_CODE=$?

            # Check if both succeeded
            if [ $SERVER_EXIT_CODE -ne 0 ]; then
              echo "Failed to create server machine"
              exit 1
            fi

            if [ $CLIENT_EXIT_CODE -ne 0 ]; then
              echo "Failed to create client machine"
              exit 1
            fi

            echo "Both machines created successfully"
          fi

          # Set outputs for other jobs to use
          echo "server_name=$SERVER_NAME" >> $GITHUB_OUTPUT
          echo "client_name=$CLIENT_NAME" >> $GITHUB_OUTPUT

  runBenchmarks:
    name: Run Benchmark ${{ matrix.config.index }}
    needs: [prepareBenchmarks, setupMachines]
    if: inputs.process_results == false
    runs-on: ubuntu-latest
    strategy:
      max-parallel: ${{ fromJSON(inputs.max_parallel) }}
      fail-fast: false
      matrix:
        config: ${{ fromJSON(needs.prepareBenchmarks.outputs.matrix) }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      - uses: webfactory/ssh-agent@v0.8.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
      - name: Run benchmark
        run: |
          export HCLOUD_TOKEN=${{ secrets.HCLOUD_TOKEN }}
          export POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}
          export POSTGRES_HOST=${{ secrets.POSTGRES_HOST }}
          export QDRANT_VERSION=${{ matrix.config.qdrant_version }}
          export DATASETS=${{ matrix.config.dataset }}
          export ENGINE_NAME=${{ matrix.config.engine_config }}
          export POSTGRES_TABLE=benchmark_manual
          export QDRANT__FEATURE_FLAGS__ALL=${{ matrix.config.feature_flags_all }}
          export CURRENT_BATCH=${{ inputs.current_batch || 0 }}
          export CURRENT_INDEX=${{ matrix.config.index }}
          export MACHINES_PER_BENCH="${{ inputs.machines_per_bench }}"

          # Use appropriate server and client names based on machines_per_bench setting
          if [ "$MACHINES_PER_BENCH" = "false" ]; then
            # Use machines_info array - from inputs for batch 1+, from prepareBenchmarks for batch 0
            if [ "$CURRENT_BATCH" != "0" ]; then
              echo "Using machines_info from inputs for batch $CURRENT_BATCH"
              MACHINES_INFO='${{ inputs.machines_info }}'
            else
              echo "Using machines_info from prepareBenchmarks for batch $CURRENT_BATCH"
              MACHINES_INFO='${{ needs.prepareBenchmarks.outputs.machines_info }}'
            fi

            JOB_INDEX=${{ strategy.job-index }}

            # Extract server and client names from machines_info array using jq
            export SERVER_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].server_name")
            export CLIENT_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].client_name")

            echo "Using machines: SERVER_NAME=$SERVER_NAME, CLIENT_NAME=$CLIENT_NAME"
          else
            export SERVER_NAME="benchmark-cascade-server-${CURRENT_INDEX}"
            export CLIENT_NAME="benchmark-cascade-client-${CURRENT_INDEX}"
            echo "Using per-benchmark machines: SERVER_NAME=$SERVER_NAME, CLIENT_NAME=$CLIENT_NAME"
          fi

          export FETCH_ALL_RESULTS="true"

          #bash -x tools/setup_ci.sh
          #bash -x tools/run_ci.sh

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.config.qdrant_version_sanitized }}-bench-${{ matrix.config.dataset }}-${{ matrix.config.engine_config }}-${{ matrix.config.index }}
          path: results/
          retention-days: 7
      - name: Teardown machines
        if: ${{ always() }}
        continue-on-error: true
        run: |
          export HCLOUD_TOKEN=${{ secrets.HCLOUD_TOKEN }}
          export CURRENT_BATCH=${{ inputs.current_batch || 0 }}
          export CURRENT_INDEX=${{ matrix.config.index }}
          export MACHINES_PER_BENCH="${{ inputs.machines_per_bench }}"
          export HAS_NEXT_BATCH="${{ needs.prepareBenchmarks.outputs.has_next_batch }}"

          # Check if we should skip machine teardown
          if [ "$MACHINES_PER_BENCH" = "false" ] && [ "$HAS_NEXT_BATCH" = "true" ]; then
            echo "machines_per_bench is false and more batches coming (has_next_batch=$HAS_NEXT_BATCH), skipping machine teardown"
            echo "Machines will be reused for next batch"
          else
            echo "Tearing down machines (machines_per_bench=$MACHINES_PER_BENCH, has_next_batch=$HAS_NEXT_BATCH)"

            if [ "$MACHINES_PER_BENCH" = "false" ]; then
              echo "Using machines_info for teardown"
              # Use machines_info array - from inputs for batch 1+, from prepareBenchmarks for batch 0
              if [ "$CURRENT_BATCH" != "0" ]; then
                MACHINES_INFO='${{ inputs.machines_info }}'
              else
                MACHINES_INFO='${{ needs.prepareBenchmarks.outputs.machines_info }}'
              fi

              JOB_INDEX=${{ strategy.job-index }}

              # Extract server and client names from machines_info array using jq
              export SERVER_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].server_name")
              export CLIENT_NAME=$(echo "$MACHINES_INFO" | jq -r ".[$JOB_INDEX].client_name")
            else
              echo "Deriving server and client names from CURRENT_INDEX"
              export SERVER_NAME="benchmark-cascade-server-${CURRENT_INDEX}"
              export CLIENT_NAME="benchmark-cascade-client-${CURRENT_INDEX}"
            fi

            bash -x tools/setup_ci.sh
            bash -x tools/tear_down.sh
          fi
  processBenchmarks:
    name: Process All Benchmark Results
    needs: [prepareBenchmarks, runBenchmarks]
    if: always() && (needs.prepareBenchmarks.outputs.has_next_batch == 'false' || inputs.process_results == true)
    runs-on: ubuntu-latest
    container:
      image: python:3.11-slim
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}
      - name: Install dependencies
        run: pip install pandas jupyter nbconvert
      - name: Download artifacts from specific workflow runs
        shell: bash
        run: |
          # Install GitHub CLI
          apt-get update && apt-get install -y curl unzip jq
          curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
          echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null
          apt-get update && apt-get install -y gh

          # Download artifacts from specific workflow runs
          mkdir -p artifacts

          # Get workflow run IDs from the accumulated list
          # If process_results is true, use the input workflow_run_ids instead
          if [ "${{ inputs.process_results }}" = "true" ]; then
            WORKFLOW_RUN_IDS="${{ inputs.workflow_run_ids }}"
            echo "Using workflow run IDs from input (process_results=true): $WORKFLOW_RUN_IDS"
          else
            WORKFLOW_RUN_IDS="${{ needs.prepareBenchmarks.outputs.workflow_run_ids }}"
            echo "Using workflow run IDs from prepareBenchmarks: $WORKFLOW_RUN_IDS"
          fi

          # Convert comma-separated list to array and download artifacts from each run
          echo "$WORKFLOW_RUN_IDS" | tr ',' '\n' | while read run_id; do
            if [ -n "$run_id" ]; then
              echo "Downloading artifacts from run $run_id"

              # List artifacts for this run
              gh api repos/${{ github.repository }}/actions/runs/$run_id/artifacts \
                --jq '.artifacts[] | select(.name | startswith("results-")) | {name: .name, url: .archive_download_url}' \
                | while IFS= read -r line; do
                    artifact_name=$(echo "$line" | jq -r '.name')
                    artifact_url=$(echo "$line" | jq -r '.url')

                    if [ -n "$artifact_url" ] && [ "$artifact_url" != "null" ]; then
                      echo "Downloading $artifact_name from run $run_id"
                      gh api "$artifact_url" > "artifacts/${run_id}-${artifact_name}.zip" || true
                    fi
                  done
            fi
          done

          # Keep zips for version parsing, don't extract yet
          ls -la artifacts/
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Upload all artifacts
        uses: actions/upload-artifact@v4
        with:
          name: all-unprocessed-results
          path: artifacts/
          retention-days: 7
      - name: Process results by Qdrant version
        run: |
          ROOT_DIR=$(pwd)
          mkdir -p final_results
          cd artifacts

          # Check if there are any zip files
          if ! ls *.zip >/dev/null 2>&1; then
            echo "No artifacts found to process"
            exit 0
          fi

          # Extract version from artifact names (between "results-" and "-bench-")
          VERSIONS=$(ls *.zip | sed -n 's/^[0-9]*-results-\(.*\)-bench-.*/\1/p' | sort -u)

          echo "Found the following Qdrant versions:"
          echo "$VERSIONS"

          # Process each version separately
          for VERSION in $VERSIONS; do
            echo "Processing version: $VERSION"

            # Create results directory for this version
            rm -rf "$ROOT_DIR/results"
            mkdir -p "$ROOT_DIR/results"

            # Extract only artifacts for this version
            for zip_file in *results-${VERSION}-bench-*.zip; do
              if [ -f "$zip_file" ]; then
                echo "Extracting $zip_file"
                unzip -o "$zip_file" -d "$ROOT_DIR/results/"
              fi
            done

            # Run Jupyter notebook
            cd "$ROOT_DIR/scripts"
            jupyter nbconvert --to notebook --execute process-benchmarks.ipynb --output process-benchmarks-executed.ipynb

            # Copy results to final_results with version suffix
            if [ -f "results.json" ]; then
              cp results.json "$ROOT_DIR/final_results/results-${VERSION}.json"
              echo "Created final_results/results-${VERSION}.json"
            else
              echo "Warning: results.json not found for version $VERSION"
            fi

            # Go back to artifacts directory for next iteration
            cd "$ROOT_DIR/artifacts"

            # Clean up results directory for next iteration
            rm -rf "$ROOT_DIR/results"
          done

          cd "$ROOT_DIR"

          echo "Final results:"
          ls -la final_results/

      - name: Upload processed results
        uses: actions/upload-artifact@v4
        with:
          name: final-processed-results
          path: final_results/
          retention-days: 7
  triggerNextBatch:
    name: Trigger Next Batch
    needs: [prepareBenchmarks, setupMachines, runBenchmarks]
    if: always() && (needs.prepareBenchmarks.outputs.has_next_batch == 'true' && inputs.process_results == false)
    runs-on: ubuntu-latest
    steps:
      - name: Trigger next batch
        run: |
          echo "Workflows ids ran so far: ${{ needs.prepareBenchmarks.outputs.workflow_run_ids }}"

          echo "🚀 Triggering next batch of benchmarks..."
          echo "Next batch index: ${{ needs.prepareBenchmarks.outputs.next_batch }}"

          # Use machines_info from prepareBenchmarks output
          MACHINES_INFO='${{ needs.prepareBenchmarks.outputs.machines_info }}'
          echo "Using machines_info from prepareBenchmarks: $MACHINES_INFO"

          PARAMS_OVERRIDE='${{ inputs.params_override }}'
          # Escape the JSON for embedding in the curl command
          ESCAPED_PARAMS=$(echo "$PARAMS_OVERRIDE" | jq -Rs .)
          ESCAPED_MACHINES_INFO=$(echo "$MACHINES_INFO" | jq -Rs .)

          # Trigger next batch via workflow dispatch
          curl -X POST \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/${{ github.repository }}/actions/workflows/manual-benchmarks-cascade.yaml/dispatches" \
            -d "{
              \"inputs\": {
                \"benchmark_set\": \"${{ needs.prepareBenchmarks.outputs.benchmark_set }}\",
                \"current_batch\": \"${{ needs.prepareBenchmarks.outputs.next_batch }}\",
                \"workflow_run_ids\": \"${{ needs.prepareBenchmarks.outputs.workflow_run_ids }}\",
                \"params_override\": $ESCAPED_PARAMS,
                \"machines_per_bench\": \"${{ inputs.machines_per_bench }}\",
                \"machines_info\": $ESCAPED_MACHINES_INFO,
                \"max_parallel\": \"${{ inputs.max_parallel || 40 }}\"
              },
              \"ref\": \"${{ github.ref }}\"
            }"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}